# Word Embedding

The Project contains the implementation of two types of word embedding models, Frequency Based and Prediction based. 

1. Frequency Based Embedding: The embeddings were generated by using a corpora specified co-occurrence matrix followed by the implementation of Singular Value Decomposition on the data. 
2. Prediction Based Embedding: The word vectors were trained using a word2vec algorithm followed by Negative Sampling to reduce the computation costs and approximate the process. 

More details regarding the implementation and results can be found in the report.

## Models

The models used can be downloaded from this [Link](https://iiitaphyd-my.sharepoint.com/:u:/g/personal/vamshi_b_research_iiit_ac_in/EVgxVLfWrwFFmK-wFbug3PEBYapNDm9JmEoRcpBhygElUg?e=yGv34I)

## directory structure 

- src
    - cmatrix_svd.py (contains the implementation for the svd model)
    - word2vec.py (contains the implementation for the word2vec model)
    - test_svd.py (svd analysis)
    - test_word2vec.py (word2vec analysis)
    - utils.py (utility functions)
- README.md
- report.pdf

To train the svd model:
`python cmatrix_svd.py`

- this saves the model in models/model1.csv
- it can be tested by changing the words in test_svd.py and running `python test_svd.py`

To train the word2vec model:
`python word2vec.py`

- it can be tested by changing the words in test_word2vec.py and running ` python test_word2vec.py`

Running either tests will output a tsne plot of the embeddings.


the word2vec model can be restored by loading it from models/model2

```py new_model = keras.models.load_model('./../models/model2') ```

will load the model in the new model from which we can access the embeddings
